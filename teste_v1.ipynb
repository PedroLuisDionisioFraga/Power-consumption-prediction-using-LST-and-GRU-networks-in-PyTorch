{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas a serem usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão do 'torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o caminho do repositório cujas informações serão usadas para treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AEP_hourly.csv', 'COMED_hourly.csv', 'DAYTON_hourly.csv', 'DEOK_hourly.csv', 'DOM_hourly.csv', 'DUQ_hourly.csv', 'EKPC_hourly.csv', 'FE_hourly.csv', 'NI_hourly.csv', 'PJME_hourly.csv', 'PJMW_hourly.csv', 'PJM_Load_hourly.csv']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um total de 12 arquivos .csv contendo dados de tendência de energia horária ('est_hourly.paruqet' e 'pjm_hourly_est.csv' não são usados). Em nossa próxima etapa, estaremos lendo esses arquivos e pré-processando esses dados nesta ordem:\n",
    "\n",
    "Obtendo os dados de tempo de cada passo de tempo individual e generalizando-os:\n",
    "\n",
    " * Hora do dia, ou seja, 0-23\n",
    " * Dia da semana, ou seja, 1-7\n",
    " * Mês, ou seja, 1-12\n",
    " * Dia do ano, ou seja, 1-365\n",
    "\n",
    "Escale os dados para valores entre 0 e 1:\n",
    " * Os algoritmos tendem a ter um desempenho melhor ou convergir mais rapidamente quando os recursos estão em uma escala relativamente semelhante e/ou próxima da distribuição normal.\n",
    " * A escala preserva a forma da distribuição original e não reduz a importância dos outliers\n",
    "\n",
    "Agrupe os dados em sequências para serem usadas como entradas para o modelo e armazene seus rótulos correspondentes.\n",
    "\n",
    " * O comprimento da sequência ou período window_size é o número de pontos de dados no histórico que o modelo usará para fazer a previsão.\n",
    " * O rótulo será o próximo ponto de dados no tempo após o último na sequência de entrada.\n",
    "As entradas e rótulos serão então divididos em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
    "  \"\"\"\n",
    "    data: matriz numpy incluindo dados\n",
    "    window_size: tamanho da janela\n",
    "    inputs_cols_indices: índices de col para incluir\n",
    "  \"\"\"\n",
    "\n",
    "  # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
    "  inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
    "  labels = np.zeros(len(data) - window_size)\n",
    "\n",
    "  for i in range(window_size, len(data)):\n",
    "    inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
    "    labels[i - window_size] = data[i, label_col_index]\n",
    "  inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
    "  labels = labels.reshape(-1, 1)\n",
    "  print(inputs.shape, labels.shape)\n",
    "\n",
    "  return inputs, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acelerar as coisas, usarei apenas arquivos num_files_for_dataset .csv para criar meu conjunto de dados. Sinta-se à vontade para executá-lo você mesmo com todo o conjunto de dados, se tiver tempo e capacidade de computação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cf42deedec4051a48ab6539b2a8f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AEP_hourly.csv ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mhour\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mhour, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mdayofweek\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdayofweek, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m df[\u001b[39m\"\u001b[39;49m\u001b[39mmonth\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmonth, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     31\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mdayofyear\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdayofyear, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3612\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3609\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3611\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3612\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3784\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3774\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3775\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3776\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3777\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3782\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3783\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3784\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   3786\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3787\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   3788\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   3789\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3790\u001b[0m     ):\n\u001b[0;32m   3791\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3792\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:4506\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4504\u001b[0m \u001b[39m# We should never get here with DataFrame value\u001b[39;00m\n\u001b[0;32m   4505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Series):\n\u001b[1;32m-> 4506\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4508\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4509\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:10771\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  10767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_reindex_for_setitem\u001b[39m(value: FrameOrSeriesUnion, index: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m  10768\u001b[0m     \u001b[39m# reindex if necessary\u001b[39;00m\n\u001b[0;32m  10770\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(index) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m> 10771\u001b[0m         \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39;49m_values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m  10773\u001b[0m     \u001b[39m# GH#4107\u001b[39;00m\n\u001b[0;32m  10774\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "label_col_index = 0  # consumption as label to predict\n",
    "inputs_cols_indices = range(5)\n",
    "# use (consumption, hour, dayofweek, month, dayofyear) columns as features\n",
    "\n",
    "# Define window_size period and split inputs/labels\n",
    "window_size = 90\n",
    "\n",
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "# Skipping the files we're not using\n",
    "processing_files = [\n",
    "  file for file in os.listdir(data_dir) if os.path.splitext(file)[1] == \".csv\"\n",
    "]\n",
    "\n",
    "num_files_for_dataset = 5\n",
    "\n",
    "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
    "  print(f\"Processing {file} ...\")\n",
    "  # Store csv file in a Pandas DataFrame\n",
    "  df = pd.read_csv(os.path.join(data_dir, file), parse_dates=[\"Datetime\"])\n",
    "\n",
    "  # Processing the time data into suitable input formats\n",
    "  df[\"hour\"] = df.apply(lambda x: x[\"Datetime\"].hour, axis=1)\n",
    "  df[\"dayofweek\"] = df.apply(lambda x: x[\"Datetime\"].dayofweek, axis=1)\n",
    "  df[\"month\"] = df.apply(lambda x: x[\"Datetime\"].month, axis=1)\n",
    "  df[\"dayofyear\"] = df.apply(lambda x: x[\"Datetime\"].dayofyear, axis=1)\n",
    "  df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
    "\n",
    "  # Scaling the input data\n",
    "  sc = MinMaxScaler()\n",
    "  label_sc = MinMaxScaler()\n",
    "  data = sc.fit_transform(df.values)\n",
    "\n",
    "  # Obtaining the scaler for the labels(usage data) so that output can be\n",
    "  # re-scaled to actual value during evaluation\n",
    "  label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
    "  label_scalers[file] = label_sc\n",
    "\n",
    "  # Move the window\n",
    "  inputs, labels = move_sliding_window(\n",
    "    data,\n",
    "    window_size,\n",
    "    inputs_cols_indices=inputs_cols_indices,\n",
    "    label_col_index=label_col_index,\n",
    "  )\n",
    "\n",
    "  # CONCAT created instances from all .csv files.\n",
    "  # Split data into train/test portions and combining all data from different files into a single array\n",
    "  test_portion = int(0.1 * len(inputs))\n",
    "  if len(train_x) == 0:  # first iteration\n",
    "    train_x = inputs[:-test_portion]\n",
    "    train_y = labels[:-test_portion]\n",
    "  else:\n",
    "    train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "    train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "    test_x[file] = inputs[-test_portion:]\n",
    "    test_y[file] = labels[-test_portion:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregadores/geradores de dados Pytorch\n",
    "Para melhorar a velocidade do nosso treinamento, podemos processar os dados em lotes para que o modelo não precise atualizar seus pesos com tanta frequência. As classes TensorDataset e DataLoader são úteis para dividir nossos dados em lotes e embaralhá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "\n",
    "# Drop the last incomplete batch\n",
    "train_loader = DataLoader(\n",
    "  train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (434274, 90, 5), Batch Size: 1024, # of iterations per epoch: 424\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liberando memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x, train_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando sua GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"GPU is available\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(GRUNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.gru = nn.GRU(\n",
    "        input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
    "    )\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    out, h = self.gru(x, h)\n",
    "    # print(out[:, -1].shape, h.shape)\n",
    "    # select hidden state of last timestamp (t=90) (1024, 256)\n",
    "    out = self.fc(self.relu(out[:, -1]))  # out[:, -1, :]\n",
    "    # print(out.shape) # (1024, 1)\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    # Initialze h_0 with zeros\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "    )\n",
    "    return hidden\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(LSTMNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
    "    )\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    out, h = self.lstm(x, h)\n",
    "    out = self.fc(self.relu(out[:, -1]))\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    # Initialze h_0, c_0 with zeros\n",
    "    hidden = (\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim)\n",
    "     .zero_()\n",
    "     .to(device),  # h_0\n",
    "     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "    )\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  train_loader,\n",
    "  learn_rate,\n",
    "  hidden_dim=256,\n",
    "  n_layers=2,\n",
    "  n_epochs=5,\n",
    "  model_type=\"GRU\",\n",
    "  print_every=100,  # Me mostra a cada 100 BATCH's\n",
    "):\n",
    "\n",
    "  input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
    "\n",
    "  # Batch generator (train_data, train_label)\n",
    "  # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
    "\n",
    "  output_dim = 1\n",
    "\n",
    "  # Instantiating the models\n",
    "  if model_type == \"GRU\":\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  else:\n",
    "    model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  model.to(device)\n",
    "\n",
    "  # Defining loss function and optimizer\n",
    "  criterion = nn.MSELoss()  # Mean Squared Error\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "  model.train()\n",
    "  print(\"Starting Training of {} model\".format(model_type))\n",
    "  epoch_times = []\n",
    "\n",
    "  # Start training loop\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.process_time()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    avg_loss = 0.0\n",
    "    counter = 0\n",
    "    for x, label in train_loader:\n",
    "      counter += 1\n",
    "      if model_type == \"GRU\":\n",
    "        h = h.data\n",
    "      # Unpcak both h_0 and c_0\n",
    "      elif model_type == \"LSTM\":\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "      # Set the gradients to zero before starting to do backpropragation because\n",
    "      # PyTorch accumulates the gradients on subsequent backward passes\n",
    "      model.zero_grad()\n",
    "\n",
    "      out, h = model(x.to(device).float(), h)\n",
    "      loss = criterion(out, label.to(device).float())\n",
    "\n",
    "      # Perform backpropragation\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      avg_loss += loss.item()\n",
    "      if counter % print_every == 0:\n",
    "        print(\n",
    "          f\"Epoch {epoch} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\"\n",
    "        )\n",
    "    current_time = time.process_time()\n",
    "\n",
    "    print(\n",
    "      f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\"\n",
    "    )\n",
    "\n",
    "    print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
    "\n",
    "    epoch_times.append(current_time - start_time)\n",
    "\n",
    "  print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1 - Step: 100/424 - Average Loss for Epoch: 0.009879196727415546\n",
      "Epoch 1 - Step: 200/424 - Average Loss for Epoch: 0.005571287257771474\n",
      "Epoch 1 - Step: 300/424 - Average Loss for Epoch: 0.003975960200186819\n",
      "Epoch 1 - Step: 400/424 - Average Loss for Epoch: 0.003132773656761856\n",
      "Epoch 1/5 Done, Total Loss: 0.0029821013190227843\n",
      "Time Elapsed for Epoch: 4400.0625 seconds\n",
      "Epoch 2 - Step: 100/424 - Average Loss for Epoch: 0.0004691695561632514\n",
      "Epoch 2 - Step: 200/424 - Average Loss for Epoch: 0.00039903040044009686\n",
      "Epoch 2 - Step: 300/424 - Average Loss for Epoch: 0.0003658177685186577\n",
      "Epoch 2 - Step: 400/424 - Average Loss for Epoch: 0.0003408717665661243\n",
      "Epoch 2/5 Done, Total Loss: 0.0003346937807674414\n",
      "Time Elapsed for Epoch: 4327.671875 seconds\n",
      "Epoch 3 - Step: 100/424 - Average Loss for Epoch: 0.00023332052092882804\n",
      "Epoch 3 - Step: 200/424 - Average Loss for Epoch: 0.00022546889260411263\n",
      "Epoch 3 - Step: 300/424 - Average Loss for Epoch: 0.00022336965829405624\n",
      "Epoch 3 - Step: 400/424 - Average Loss for Epoch: 0.00021599445775791537\n",
      "Epoch 3/5 Done, Total Loss: 0.00021360680172088082\n",
      "Time Elapsed for Epoch: 4513.40625 seconds\n",
      "Epoch 4 - Step: 100/424 - Average Loss for Epoch: 0.00018918376765213906\n",
      "Epoch 4 - Step: 200/424 - Average Loss for Epoch: 0.00018198846504674293\n",
      "Epoch 4 - Step: 300/424 - Average Loss for Epoch: 0.00017678381545313944\n",
      "Epoch 4 - Step: 400/424 - Average Loss for Epoch: 0.00017620437607547502\n",
      "Epoch 4/5 Done, Total Loss: 0.0001749049957343575\n",
      "Time Elapsed for Epoch: 4333.53125 seconds\n",
      "Epoch 5 - Step: 100/424 - Average Loss for Epoch: 0.00016806868217827286\n",
      "Epoch 5 - Step: 200/424 - Average Loss for Epoch: 0.0001569887701407424\n",
      "Epoch 5 - Step: 300/424 - Average Loss for Epoch: 0.00015697256050771103\n",
      "Epoch 5 - Step: 400/424 - Average Loss for Epoch: 0.00015232635341817514\n",
      "Epoch 5/5 Done, Total Loss: 0.00015077041405434874\n",
      "Time Elapsed for Epoch: 4119.96875 seconds\n",
      "Total Training Time: 21694.640625 seconds\n"
     ]
    }
   ],
   "source": [
    "# seq_len = 90  # (timestamps)\n",
    "n_hidden = 256\n",
    "n_layers = 2\n",
    "n_epochs = 5\n",
    "print_every = 100\n",
    "lr = 0.001\n",
    "gru_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"GRU\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model.state_dict(), \"./models/gru_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando e salvando um modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of LSTM model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_model \u001b[39m=\u001b[39m train(\n\u001b[0;32m      2\u001b[0m   train_loader,\n\u001b[0;32m      3\u001b[0m   learn_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m      4\u001b[0m   hidden_dim\u001b[39m=\u001b[39;49mn_hidden,\n\u001b[0;32m      5\u001b[0m   n_layers\u001b[39m=\u001b[39;49mn_layers,\n\u001b[0;32m      6\u001b[0m   n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[0;32m      7\u001b[0m   model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLSTM\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m   print_every\u001b[39m=\u001b[39;49mprint_every,\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, learn_rate, hidden_dim, n_layers, n_epochs, model_type, print_every)\u001b[0m\n\u001b[0;32m     52\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, label\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m     54\u001b[0m \u001b[39m# Perform backpropragation\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     56\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    238\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    239\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    244\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 245\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 145\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    146\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    147\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"LSTM\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), \"./models/lstm_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 5\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "lstm_model.load_state_dict(torch.load(\"./models/lstm_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAPE(outputs, targets):\n",
    "  sMAPE = (\n",
    "    100\n",
    "    / len(targets)\n",
    "    * np.sum(np.abs(outputs - targets) / (np.abs(outputs + targets)) / 2)\n",
    "  )\n",
    "  return sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "  model.eval()\n",
    "  outputs = []\n",
    "  targets = []\n",
    "  start_time = time.process_time()\n",
    "  # get data of test data for each state\n",
    "  for file in test_x.keys():\n",
    "    inputs = torch.from_numpy(np.array(test_x[file]))\n",
    "    labels = torch.from_numpy(np.array(test_y[file]))\n",
    "\n",
    "    h = model.init_hidden(inputs.shape[0])\n",
    "\n",
    "    # predict outputs\n",
    "    with torch.no_grad():\n",
    "      out, h = model(inputs.to(device).float(), h)\n",
    "\n",
    "    outputs.append(\n",
    "      label_scalers[file]\n",
    "      .inverse_transform(out.cpu().detach().numpy())\n",
    "      .reshape(-1)\n",
    "    )\n",
    "\n",
    "    targets.append(\n",
    "      label_scalers[file].inverse_transform(labels.numpy()).reshape(-1)\n",
    "    )\n",
    "\n",
    "  # Merge all files\n",
    "  concatenated_outputs = np.concatenate(outputs)\n",
    "  concatenated_targets = np.concatenate(targets)\n",
    "\n",
    "  print(f\"Evaluation Time: {time.process_time()-start_time}\")\n",
    "  print(f\"sMAPE: {round(sMAPE(concatenated_outputs, concatenated_targets), 3)}%\")\n",
    "\n",
    "  # list of of targets/outputs for each state\n",
    "  return outputs, targets, sMAPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie o desempenho da GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie o desempenho do LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\n",
    "  gru_outputs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo algumas visualizações em conjuntos aleatórios de nossa saída prevista versus os dados de consumo real para alguns estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = list(test_x.keys())\n",
    "states_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(gru_outputs[0][-100:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(\n",
    "  lstm_outputs[0][-100:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2\n",
    ")\n",
    "plt.plot(targets[0][-100:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[0]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(gru_outputs[1][-50:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[1][-50:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[1][-50:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[1]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(gru_outputs[2][:50], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[2][:50], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[2][:50], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[2]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(gru_outputs[3][:100], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[3][:100], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[3][:100], color=\"b\", label=\"Actual\")\n",
    "plt.title(f\"Energy Consumption for {states_list[3]} state\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

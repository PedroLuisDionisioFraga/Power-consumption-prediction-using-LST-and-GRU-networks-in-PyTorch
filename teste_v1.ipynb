{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas a serem usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão do 'torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o dataset\n",
    "\n",
    "#O arquivo possui duas planilhas, uma referente ao número de unidades consumidoras e outra ao\n",
    "#consumo em MWh por região, por isto \n",
    "\n",
    "#Atribuindo nossa base de dados à um DataFrame da biblioteca Pandas\n",
    "\n",
    "df = pd.read_excel('models/municipio_mensal.xlsx', sheet_name='Consumo MWh')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vendo suas dimensões e seus primeiros 10 itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4512, 353)\n",
      "Primeiros 10 itens\n",
      "     tipo  cod_muni agência                 núcleo               unidade  \\\n",
      "0  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "1  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "2  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "3  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "4  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "5  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "6  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "7  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "8  Cativo      1101   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "9  Cativo      1102   ARFLO  Núcleo Grande Capital  Florianópolis (sede)   \n",
      "\n",
      "       município  cod_classe              classe  1994-01-01 00:00:00  \\\n",
      "0  Florianópolis           1         Residencial            21357.147   \n",
      "1  Florianópolis           2          Industrial             1698.383   \n",
      "2  Florianópolis           3           Comercial            14766.175   \n",
      "3  Florianópolis           4               Rural               25.908   \n",
      "4  Florianópolis           5       Poder Público             6393.112   \n",
      "5  Florianópolis           6  Iluminação Pública             2513.127   \n",
      "6  Florianópolis           7     Serviço Público              659.419   \n",
      "7  Florianópolis           8             Próprio              419.640   \n",
      "8  Florianópolis           9             Revenda                0.000   \n",
      "9       São José           1         Residencial             7482.370   \n",
      "\n",
      "   1994-02-01 00:00:00  ...  2021-12-01 00:00:00  2022-01-01 00:00:00  \\\n",
      "0            20782.785  ...          51207.82046            67805.186   \n",
      "1             1641.103  ...           2976.46100             3743.179   \n",
      "2            13420.879  ...          31813.04400            36778.731   \n",
      "3               23.863  ...             42.85900               50.487   \n",
      "4             4903.650  ...           6690.84000             7521.586   \n",
      "5             2568.735  ...           3182.78600             3307.835   \n",
      "6              653.971  ...           2840.97200             3340.023   \n",
      "7              404.616  ...            346.17300              419.065   \n",
      "8                0.000  ...              0.00000                0.000   \n",
      "9             6657.775  ...          22181.04467            26037.616   \n",
      "\n",
      "   2022-02-01 00:00:00  2022-03-01 00:00:00  2022-04-01 00:00:00  \\\n",
      "0          72458.04394          71901.73800            54816.429   \n",
      "1           3822.83400           3799.09100             3305.182   \n",
      "2          40113.04500          39851.61500            35448.770   \n",
      "3             50.48700             49.42300               43.292   \n",
      "4           8527.39900           8653.28200             8906.695   \n",
      "5           3331.57000           3015.39300             3355.822   \n",
      "6           3314.49200           2956.27800             3174.170   \n",
      "7            502.88200            439.65800              460.417   \n",
      "8              0.00000              0.00000                0.000   \n",
      "9          28203.81981          28973.16798            22816.618   \n",
      "\n",
      "   2022-05-01 00:00:00  2022-06-01 00:00:00  2022-07-01 00:00:00  \\\n",
      "0            49513.315            51371.377          50536.92089   \n",
      "1             2963.088             2923.234           2894.99300   \n",
      "2            30309.031            27145.299          26488.54700   \n",
      "3               43.960               44.956             43.09200   \n",
      "4             7159.980             6648.814           6152.60000   \n",
      "5             3237.200             3452.284           3281.90700   \n",
      "6             2881.616             3081.155           2628.84900   \n",
      "7              372.993              295.365            259.84500   \n",
      "8                0.000                0.000              0.00000   \n",
      "9            20837.235            21523.631          21099.49300   \n",
      "\n",
      "   2022-08-01 00:00:00  2022-09-01 00:00:00  \n",
      "0          50106.64000            50833.496  \n",
      "1           2877.98100             2860.292  \n",
      "2          27398.64625            25944.034  \n",
      "3             44.89600               54.093  \n",
      "4           6417.01300             6340.768  \n",
      "5           3605.18900             3525.253  \n",
      "6           2735.45900             2822.698  \n",
      "7            272.41600              265.046  \n",
      "8              0.00000                0.000  \n",
      "9          21298.87600            21263.456  \n",
      "\n",
      "[10 rows x 353 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Primeiros 10 itens\\n{df.head(10)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos retirar as colunas que não nos interessam, como:\n",
    " * tipo \n",
    " * cod_muni\n",
    " * agência\n",
    " * núcleo\n",
    " * unidade\n",
    " * cod_classe\n",
    " * município (Após a filtragem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantendo no DataFrame apenas os dados com município igual a Florianópolis\n",
    "df = df.loc[df[\"município\"] == \"Florianópolis\"]\n",
    "# Removendo as colunas que não serão utilizadas em nossa análise\n",
    "df = df.drop(columns=['tipo', 'cod_muni', 'agência', 'núcleo', 'unidade', 'cod_classe', 'município'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora possuímos apenas dados referentes ao município de Florianópolis. Porém a distribuição destes dados no DataFrame não está de forma ideal para analisá-los. Utilizaremos a função **melt()** para despivotar a tabela utilizando a coluna classe como referência, assim alterando o formato de **wide** para **long**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizando a feature classe como ponto de pivotamento, assim tendo o DataFrame corretamente\n",
    "#construido\n",
    "df_floripa = df.melt(id_vars=['classe'])\n",
    "\n",
    "#Renomeando as colunas restantes para refletir o que estamos analisando\n",
    "df_floripa.rename(columns = {'variable':'Data','value':'MWh'}, inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos separar os dados de consumo em relação a sua classe, criando um DataFrame para cada uma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando os dados de consumo pela classe em diferentes DataFrames\n",
    "df_res = df_floripa.loc[df_floripa['classe']=='Residencial']\n",
    "df_ind = df_floripa.loc[df_floripa['classe']=='Industrial']\n",
    "df_com = df_floripa.loc[df_floripa['classe']=='Comercial']\n",
    "df_rur = df_floripa.loc[df_floripa['classe']=='Rural']\n",
    "df_pub = df_floripa.loc[df_floripa['classe']=='Poder Público']\n",
    "df_ilu = df_floripa.loc[df_floripa['classe']=='Iluminação Pública']\n",
    "df_ser = df_floripa.loc[df_floripa['classe']=='Serviço Público']\n",
    "df_pro = df_floripa.loc[df_floripa['classe']=='Próprio']\n",
    "\n",
    "#print(df_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando ainda mais\n",
    " * Removendo a coluna \"classe\".\n",
    " * Somando todos os valores da coluna \"MWh\" que possuem as datas da coluna \"Data\" iguais.\n",
    "\n",
    "Como podemos perceber acima, as linhas de consumo se repetem, mesmo pertencendo a mesma classe. Assim utilizaremos a função utilizando as funções **groupby()** e **sum()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.groupby(['Data'], as_index=False).sum()\n",
    "df_ind = df_ind.groupby(['Data'], as_index=False).sum()\n",
    "df_com = df_com.groupby(['Data'], as_index=False).sum()\n",
    "df_rur = df_rur.groupby(['Data'], as_index=False).sum()\n",
    "df_pub = df_pub.groupby(['Data'], as_index=False).sum()\n",
    "df_ilu = df_ilu.groupby(['Data'], as_index=False).sum()\n",
    "df_ser = df_ser.groupby(['Data'], as_index=False).sum()\n",
    "df_pro = df_pro.groupby(['Data'], as_index=False).sum()\n",
    "#print(df_com)\n",
    "\n",
    "#print(df_res[\"Data\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renomeando a coluna \"Data\" para \"Datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df_res.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_ind = df_ind.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_com = df_com.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_rur = df_rur.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_pub = df_pub.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_ilu = df_ilu.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_ser = df_ser.rename(columns={\"Data\": \"Datetime\"})\n",
    "df_pro = df_pro.rename(columns={\"Data\": \"Datetime\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo para o formato \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_concat = pd.concat([df_res[\"Datetime\"], df_res[\"MWh\"]], axis=1)\n",
    "df_ind_concat = pd.concat([df_ind[\"Datetime\"], df_ind[\"MWh\"]], axis=1)\n",
    "df_com_concat = pd.concat([df_com[\"Datetime\"], df_com[\"MWh\"]], axis=1)\n",
    "df_rur_concat = pd.concat([df_rur[\"Datetime\"], df_rur[\"MWh\"]], axis=1)\n",
    "df_pub_concat = pd.concat([df_pub[\"Datetime\"], df_pub[\"MWh\"]], axis=1)\n",
    "df_ilu_concat = pd.concat([df_ilu[\"Datetime\"], df_ilu[\"MWh\"]], axis=1)\n",
    "df_ser_concat = pd.concat([df_ser[\"Datetime\"], df_ser[\"MWh\"]], axis=1)\n",
    "df_pro_concat = pd.concat([df_pro[\"Datetime\"], df_pro[\"MWh\"]], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Convertendo no formato \".csv\"\n",
    "3) Salvando os modelos filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando as alterações\n",
    "df_res_concat.to_csv(\"models/After Filters/res_municipio_mensal.csv\", index=False)\n",
    "df_ind_concat.to_csv(\"models/After Filters/ind_municipio_mensal.csv\", index=False)\n",
    "df_com_concat.to_csv(\"models/After Filters/com_municipio_mensal.csv\", index=False)\n",
    "df_rur_concat.to_csv(\"models/After Filters/rur_municipio_mensal.csv\", index=False)\n",
    "df_pub_concat.to_csv(\"models/After Filters/pub_municipio_mensal.csv\", index=False)\n",
    "df_ilu_concat.to_csv(\"models/After Filters/ilu_municipio_mensal.csv\", index=False)\n",
    "df_ser_concat.to_csv(\"models/After Filters/ser_municipio_mensal.csv\", index=False)\n",
    "df_pro_concat.to_csv(\"models/After Filters/pro_municipio_mensal.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definir diretório raiz de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['com_municipio_mensal.csv', 'ilu_municipio_mensal.csv', 'ind_municipio_mensal.csv', 'pro_municipio_mensal.csv', 'pub_municipio_mensal.csv', 'res_municipio_mensal.csv', 'rur_municipio_mensal.csv', 'ser_municipio_mensal.csv']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"models/After Filters\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos um total de 12 arquivos .csv contendo dados de tendência de energia horária ('est_hourly.paruqet' e 'pjm_hourly_est.csv' não são usados). Em nossa próxima etapa, estaremos lendo esses arquivos e pré-processando esses dados nesta ordem:\n",
    "\n",
    "Obtendo os dados de tempo de cada passo de tempo individual e generalizando-os:\n",
    "\n",
    " * Hora do dia, ou seja, 0-23\n",
    " * Dia da semana, ou seja, 1-7\n",
    " * Mês, ou seja, 1-12\n",
    " * Dia do ano, ou seja, 1-365\n",
    "\n",
    "Escale os dados para valores entre 0 e 1:\n",
    " * Os algoritmos tendem a ter um desempenho melhor ou convergir mais rapidamente quando os recursos estão em uma escala relativamente semelhante e/ou próxima da distribuição normal.\n",
    " * A escala preserva a forma da distribuição original e não reduz a importância dos outliers\n",
    "\n",
    "Agrupe os dados em sequências para serem usadas como entradas para o modelo e armazene seus rótulos correspondentes.\n",
    "\n",
    " * O comprimento da sequência ou período window_size é o número de pontos de dados no histórico que o modelo usará para fazer a previsão.\n",
    " * O rótulo será o próximo ponto de dados no tempo após o último na sequência de entrada.\n",
    "As entradas e rótulos serão então divididos em conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
    "  \"\"\"\n",
    "    data: matriz numpy incluindo dados\n",
    "    window_size: tamanho da janela\n",
    "    inputs_cols_indices: índices de col para incluir\n",
    "  \"\"\"\n",
    "\n",
    "  # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
    "  inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
    "  labels = np.zeros(len(data) - window_size)\n",
    "\n",
    "  for i in range(window_size, len(data)):\n",
    "    inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
    "    labels[i - window_size] = data[i, label_col_index]\n",
    "  inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
    "  labels = labels.reshape(-1, 1)\n",
    "  print(inputs.shape, labels.shape)\n",
    "\n",
    "  return inputs, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acelerar as coisas, usarei apenas arquivos num_files_for_dataset .csv para criar meu conjunto de dados. Sinta-se à vontade para executá-lo você mesmo com todo o conjunto de dados, se tiver tempo e capacidade de computação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7669d571e40e586b3cf2597ed93c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing com_municipio_mensal.csv ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m sc \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m     36\u001b[0m label_sc \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 37\u001b[0m data \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mfit_transform(df\u001b[39m.\u001b[39;49mvalues)\n\u001b[0;32m     39\u001b[0m \u001b[39m# Obtaining the scaler for the labels(usage data) so that output can be\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# re-scaled to actual value during evaluation\u001b[39;00m\n\u001b[0;32m     41\u001b[0m label_sc\u001b[39m.\u001b[39mfit(df\u001b[39m.\u001b[39miloc[:, label_col_index]\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:416\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:453\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    452\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 453\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    454\u001b[0m     X,\n\u001b[0;32m    455\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    456\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    457\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    458\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    459\u001b[0m )\n\u001b[0;32m    461\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    462\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Pedro\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Timestamp'"
     ]
    }
   ],
   "source": [
    "label_col_index = 0  # consumption as label to predict\n",
    "inputs_cols_indices = range(5)\n",
    "# use (consumption, hour, dayofweek, month, dayofyear) columns as features\n",
    "\n",
    "# Define window_size period and split inputs/labels\n",
    "window_size = 90\n",
    "\n",
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "# Skipping the files we're not using\n",
    "processing_files = [\n",
    "  file for file in os.listdir(data_dir) if os.path.splitext(file)[1] == \".csv\"\n",
    "]\n",
    "\n",
    "num_files_for_dataset = 5\n",
    "\n",
    "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
    "  print(f\"Processing {file} ...\")\n",
    "  # Store csv file in a Pandas DataFrame\n",
    "  df = pd.read_csv(os.path.join(data_dir, file), parse_dates=[\"Datetime\"])\n",
    "\n",
    "  # ARRUMA A DATA CRIANDO UMA ALEATORIA\n",
    "  # Processing the time data into suitable input formats\n",
    "  # df[\"hour\"] = df.apply(lambda x: x[\"Datetime\"].hour, axis=1)\n",
    "  # df[\"dayofweek\"] = df.apply(lambda x: x[\"Datetime\"].dayofweek, axis=1)\n",
    "  # df[\"month\"] = df.apply(lambda x: x[\"Datetime\"].month, axis=1)\n",
    "  # df[\"dayofyear\"] = df.apply(lambda x: x[\"Datetime\"].dayofyear, axis=1)\n",
    "  # df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
    "\n",
    "  # Scaling the input data\n",
    "  sc = MinMaxScaler()\n",
    "  label_sc = MinMaxScaler()\n",
    "  data = sc.fit_transform(df.values)\n",
    "\n",
    "  # Obtaining the scaler for the labels(usage data) so that output can be\n",
    "  # re-scaled to actual value during evaluation\n",
    "  label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
    "  label_scalers[file] = label_sc\n",
    "\n",
    "  # Move the window\n",
    "  inputs, labels = move_sliding_window(\n",
    "    data,\n",
    "    window_size,\n",
    "    inputs_cols_indices=inputs_cols_indices,\n",
    "    label_col_index=label_col_index,\n",
    "  )\n",
    "\n",
    "  # CONCAT created instances from all .csv files.\n",
    "  # Split data into train/test portions and combining all data from different files into a single array\n",
    "  test_portion = int(0.1 * len(inputs))\n",
    "  if len(train_x) == 0:  # first iteration\n",
    "    train_x = inputs[:-test_portion]\n",
    "    train_y = labels[:-test_portion]\n",
    "  else:\n",
    "    train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "    train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "    test_x[file] = inputs[-test_portion:]\n",
    "    test_y[file] = labels[-test_portion:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregadores/geradores de dados Pytorch\n",
    "Para melhorar a velocidade do nosso treinamento, podemos processar os dados em lotes para que o modelo não precise atualizar seus pesos com tanta frequência. As classes TensorDataset e DataLoader são úteis para dividir nossos dados em lotes e embaralhá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "\n",
    "# Drop the last incomplete batch\n",
    "train_loader = DataLoader(\n",
    "  train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "  f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liberando memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x, train_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando sua GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is available\n",
      "O tipo do device é cpu\n"
     ]
    }
   ],
   "source": [
    "# Verificar se uma GPU está disponível no sistema\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Se tivermos uma GPU disponível, configuraremos nosso dispositivo para GPU. Usaremos essa variável de dispositivo posteriormente em nosso código.\n",
    "#device = torch.device(\"cuda\")  # Use isso para obrigar a IA rodar na GPU\n",
    "if is_cuda:\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"GPU is available\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print(\"CPU is available\")\n",
    "print(f\"O tipo do device é {device.type}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Implementa, na classe GRUNet, uma rede neural GRU com uma camada totalmente conectada (fully-connected) na saída, enquanto a classe LSTMNet implementa uma rede neural LSTM com a mesma camada totalmente conectada na saída. Ambos os modelos recebem uma entrada x e um estado oculto inicial h e retornam uma saída out e um estado oculto final h.\n",
    " * As funções \"init_hidden\" são responsáveis por inicializar o estado oculto inicial com zeros para ser usado na primeira etapa de previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(GRUNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    # Inicializa a camada GRU com as dimensões fornecidas e as opções de dropout\n",
    "    self.gru = nn.GRU(\n",
    "        input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
    "    )\n",
    "    # Inicializa a camada fully connected com as dimensões fornecidas\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    # Inicializa a função de ativação ReLU\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    # Executa a camada GRU com entrada x e estado oculto h\n",
    "    out, h = self.gru(x, h)\n",
    "    # print(out[:, -1].shape, h.shape)\n",
    "    # Seleciona o estado oculto correspondente ao último timestamp da sequência (t=90) (1024, 256)\n",
    "    out = self.fc(self.relu(out[:, -1]))  # out[:, -1, :]\n",
    "    # print(out.shape) # (1024, 1)\n",
    "    # Retorna a saída e o estado oculto\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    # Inicializa o estado oculto h_0 com zeros\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "    )\n",
    "    return hidden\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(LSTMNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    # Inicializa a camada LSTM com as dimensões fornecidas e as opções de dropout\n",
    "    self.lstm = nn.LSTM(\n",
    "      input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
    "    )\n",
    "    # Inicializa a camada fully connected com as dimensões fornecidas\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    # Inicializa a função de ativação ReLU\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    # Executa a camada LSTM com entrada x e estado oculto h\n",
    "    out, h = self.lstm(x, h)\n",
    "    # Seleciona o estado oculto correspondente ao último timestamp da sequência\n",
    "    out = self.fc(self.relu(out[:, -1]))\n",
    "    # Retorna a saída e o estado oculto\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    # Inicializa o estado oculto h_0 e o estado da célula c_0 com zeros\n",
    "    hidden = (\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim)\n",
    "     .zero_()\n",
    "     .to(device),  # h_0\n",
    "     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "    )\n",
    "    return hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função train é responsável por treinar o modelo com os dados de treinamento fornecidos pelo train_loader, com o objetivo de minimizar a função de perda nn.MSELoss() (Erro Quadrático Médio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  train_loader,\n",
    "  learn_rate,\n",
    "  hidden_dim=256,\n",
    "  n_layers=2,  # Número de camadas da rede\n",
    "  n_epochs=5,  # Número de épocas\n",
    "  model_type=\"GRU\",  # Tipo de modelo\n",
    "  print_every=100,  # Me mostra a cada 100 BATCH's\n",
    "):\n",
    "  \n",
    "  # Obtendo o tamanho da entrada a partir do primeiro batch do dataloader\n",
    "  input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
    "\n",
    "  # Batch generator (train_data, train_label)\n",
    "  # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
    "\n",
    "  # Definindo a saída como uma única dimensão\n",
    "  output_dim = 1\n",
    "\n",
    "  # Instanciando o modelo, escolhendo entre GRU e LSTM\n",
    "  if model_type == \"GRU\":\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  else:\n",
    "    model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  \n",
    "  # Movendo o modelo para a GPU, se estiver disponível\n",
    "  model.to(device)\n",
    "\n",
    "  # Definindo a função de perda como Mean Squared Error (Erro Médio Quadrático)\n",
    "  criterion = nn.MSELoss()  # Mean Squared Error\n",
    "  # Definindo o otimizador como Adam com a taxa de aprendizado especificada\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "  # Colocando o modelo em modo de treinamento\n",
    "  model.train()\n",
    "  # Imprimindo mensagem de início do treinamento\n",
    "  print(\"Starting Training of {} model\".format(model_type))\n",
    "  epoch_times = []\n",
    "\n",
    "  # Iniciando o loop de treinamento por épocas\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Inicializando o hidden state\n",
    "    h = model.init_hidden(batch_size)\n",
    "    avg_loss = 0.0\n",
    "    counter = 0\n",
    "\n",
    "    # Iterando pelos batches do dataloader\n",
    "    for x, label in train_loader:\n",
    "      counter += 1\n",
    "\n",
    "      # Reinicializando o hidden state, se necessário\n",
    "      if model_type == \"GRU\":\n",
    "        h = h.data\n",
    "      # Unpack both h_0 and c_0\n",
    "      elif model_type == \"LSTM\":\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "      # Zerando os gradientes antes de executar o backpropagation\n",
    "      # pois o PyTorch acumula os gradientes nas passadas subsequentes para trás\n",
    "      model.zero_grad()\n",
    "\n",
    "      # Passando a entrada pelo modelo\n",
    "      out, h = model(x.to(device).float(), h)\n",
    "      # Calculando a perda\n",
    "      loss = criterion(out, label.to(device).float())\n",
    "\n",
    "      # Perform backpropragation\n",
    "      # Executando o backpropagation\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculando a média das perdas\n",
    "      avg_loss += loss.item()\n",
    "\n",
    "      # Imprimindo o status do treinamento a cada 'print_every' batches\n",
    "      if counter % print_every == 0:\n",
    "        print(\n",
    "          f\"Epoch {epoch} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\"\n",
    "        )\n",
    "    current_time = time.process_time()\n",
    "\n",
    "    # Imprimindo informações sobre a época atual\n",
    "    print(f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\")\n",
    "    print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
    "\n",
    "    epoch_times.append(current_time - start_time)\n",
    "\n",
    "  # Imprimindo informações sobre o tempo total de treinamento\n",
    "  print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
    "\n",
    "  # Retornando o modelo treinado\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis usadas para treinar os RMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = 90  # (timestamps)\n",
    "n_hidden = 256\n",
    "n_layers = 2\n",
    "n_epochs = 1\n",
    "print_every = 100\n",
    "lr = 0.001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"GRU\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model.state_dict(), \"./models/gru_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando e salvando um modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"LSTM\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), \"./models/lstm_model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 5\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "lstm_model.load_state_dict(torch.load(\"./models/lstm_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAPE(outputs, targets):\n",
    "  sMAPE = (\n",
    "    100\n",
    "    / len(targets)\n",
    "    * np.sum(np.abs(outputs - targets) / (np.abs(outputs + targets)) / 2)\n",
    "  )\n",
    "  return sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "  model.eval()\n",
    "  outputs = []\n",
    "  targets = []\n",
    "  start_time = time.process_time()\n",
    "  # get data of test data for each state\n",
    "  for file in test_x.keys():\n",
    "    inputs = torch.from_numpy(np.array(test_x[file]))\n",
    "    labels = torch.from_numpy(np.array(test_y[file]))\n",
    "\n",
    "    h = model.init_hidden(inputs.shape[0])\n",
    "\n",
    "    # predict outputs\n",
    "    with torch.no_grad():\n",
    "      out, h = model(inputs.to(device).float(), h)\n",
    "\n",
    "    outputs.append(\n",
    "      label_scalers[file]\n",
    "      .inverse_transform(out.cpu().detach().numpy())\n",
    "      .reshape(-1)\n",
    "    )\n",
    "\n",
    "    targets.append(\n",
    "      label_scalers[file].inverse_transform(labels.numpy()).reshape(-1)\n",
    "    )\n",
    "\n",
    "  # Merge all files\n",
    "  concatenated_outputs = np.concatenate(outputs)\n",
    "  concatenated_targets = np.concatenate(targets)\n",
    "\n",
    "  print(f\"Evaluation Time: {time.process_time()-start_time}\")\n",
    "  print(f\"sMAPE: {round(sMAPE(concatenated_outputs, concatenated_targets), 3)}%\")\n",
    "\n",
    "  # list of of targets/outputs for each state\n",
    "  return outputs, targets, sMAPE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie o desempenho da GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalie o desempenho do LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gru_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo algumas visualizações em conjuntos aleatórios de nossa saída prevista versus os dados de consumo real para alguns estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = list(test_x.keys())\n",
    "states_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(gru_outputs[0][-100:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[0][-100:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[0][-100:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[0]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(gru_outputs[1][-50:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[1][-50:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[1][-50:], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[1]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(gru_outputs[2][:50], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[2][:50], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[2][:50], color=\"b\", label=\"Actual\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.title(f\"Energy Consumption for {states_list[2]} state\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(gru_outputs[3][:100], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[3][:100], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[3][:100], color=\"b\", label=\"Actual\")\n",
    "plt.title(f\"Energy Consumption for {states_list[3]} state\")\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "Embora o modelo GRU possa ter cometido erros menores e superado ligeiramente o modelo LSTM em termos de sMAPE (erro percentual médio absoluto simétrico), a diferença é insignificante e, portanto, inconclusiva. Houve muitos outros testes conduzidos por outros comparando esses dois modelos, mas não houve um vencedor claro sobre qual é a melhor arquitetura geral. Parece que os modelos são amplamente bem-sucedidos em prever as tendências do consumo de energia. Embora eles ainda possam errar algumas mudanças, como atrasos na previsão de uma queda no consumo, as previsões seguem muito de perto a linha real no conjunto de teste. Isso se deve à natureza dos dados de consumo de energia e ao fato de que existem padrões e mudanças cíclicas que o modelo pode considerar. Problemas de previsão de séries temporais mais difíceis, como previsão de preço de ações ou previsão de volume de vendas, podem ter dados que são amplamente aleatórios ou não têm padrões previsíveis e, nesses casos, a precisão será definitivamente menor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
